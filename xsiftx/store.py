"""
This handles file uploads and hashing for placing the
file on s3 using the edX platform settings
"""
from gzip import GzipFile
import json
import hashlib
import urllib

from boto.s3.connection import S3Connection
from boto.s3.key import Key
class FSStore(object):
    """
    This writes out the file to a local path
    """
    def __init__(self, settings):
        self.root_path = settings['root_path']

    def path_for(self, course_id, filename):
        """
        This returns the path to write the file to
        """
        return os.path.join(self.root_path,
                            urllib.quote(course_id, safe=''),
                            filename)

    def store(self, course_id, filename, srcfile):
        """
        Actually writes out the file from wherever srcfile has been 
        seeked to.
        """
        full_path = self.path_for(course_id, filename)
        directory = os.path.dirname(full_path)
        if not os.path.exists(directory):
            os.mkdir(directory)
        with open(full_path, "wb") as f:
            f.write(buff.read())

class S3Store(object):
    """
    This manages the connection and uploading of files
    generated by the sifter
    """
    def __init__(self, settings):
        self.root_path = settings['root_path']

        conn = S3Connection(
            settings['aws_key_id'],
            settings['aws_key']
        )
        self.bucket = conn.get_bucket(settings['bucket'])

    def key_for(self, course_id, filename):
        """
        Return the S3 key we would use to store and retrive the data for the
        given filename.
        """
        hashed_course_id = hashlib.sha1(course_id)

        key = Key(self.bucket)
        key.key = "{}/{}/{}".format(
            self.root_path,
            hashed_course_id.hexdigest(),
            filename
        )
        return key

    def store(self, course_id, filename, srcfile):
        """
        This actually stores the file into s3
        """

        key = self.key_for(course_id, filename)

        # gzip data to optimize upload
        output_buffer = StringIO()
        gzip_file = GzipFile(fileobj=output_buffer, mode="wb")
        gzip_file.write(srcfile.read)

        data = output_buffer.getvalue()
        key.size = len(data)
        key.content_encoding = "gzip"
        key.content_type = "text/csv"

        # Just setting the content encoding and type above should work
        # according to the docs, but when experimenting, this was necessary for
        # it to actually take.
        key.set_contents_from_string(
            data,
            headers={
                "Content-Encoding": "gzip",
                "Content-Length": len(data),
                "Content-Type": "text/csv",
            }
        )
